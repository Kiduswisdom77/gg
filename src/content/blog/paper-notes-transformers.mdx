---
title: "Paper Notes: Attention Is All You Need"
date: "2023-10-15"
excerpt: "Notes on the seminal Transformers paper that revolutionized NLP."
---

The [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper introduced the Transformer architecture, which has become the foundation for most state-of-the-art NLP models today.

## Key Contributions

### Self-Attention Mechanism

The paper introduced scaled dot-product attention:
